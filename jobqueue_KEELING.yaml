distributed:
  dashboard:
    link: "/proxy/{port}/status"
  scheduler:
    bandwidth: 1000000000     # GB MB/s estimated worker-worker bandwidth
  worker:
    memory:
      target: 0.90  # Avoid spilling to disk
      spill: False  # Avoid spilling to disk
      pause: 0.90  # fraction at which we pause worker threads
      terminate: 0.98  # fraction at which we terminate the worker
  comm:
    compression: null

jobqueue:
   slurm:
     name: dask-worker

     # Dask worker options
     cores: 4                # Total number of cores per job
     memory: "10 GB"                # Total amount of memory per job
     processes: 1                # Number of Python processes per job
     allowed-failures: 3 
#    interface: ib0            #use for g partition 
#     interface: lo           #use for h partition
     death-timeout: 60           # Number of seconds to wait if a worker can not find a scheduler
     local-directory:  /data/keeling/a/YOUR-USER-ID/tmp/       # Location of fast local storage like /scratch or $TMPDIR

     # SLURM resource manager options
     queue: "seseml" 
     walltime: '00:30:00'
     worker_extra_args: [] 
     job_script_prologue: []
     job-cpu: null
     job-mem: null
#     job-extra: {'-C g20'}    #use for g partition
#     job-extra: {'--ntasks-per-node=1'}
#     job-extra: {'-C g256'}    #use for g partition-large memory nodes
#     job-extra: {'-C h20'}   #use for h partition
     log-directory: /data/keeling/a/YOUR-USER-ID/dask/

